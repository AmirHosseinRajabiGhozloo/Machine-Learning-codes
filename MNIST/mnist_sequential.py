# -*- coding: utf-8 -*-
"""MNIST_Sequential.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FKGTg0ooO1y-8eTYxLj6rkldfkdPyavB
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers, Model

# import dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# flatten the features from 28*28 pixel to 784 wide vector
x_train = np.reshape(x_train, (-1, 784)).astype('float32')
x_test = np.reshape(x_test, (-1, 784)).astype('float32')

# one-hot encode the targets
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

data_slice = 10000
x_train = x_train[:data_slice,:]
y_train = y_train[:data_slice,:]
x_test = x_test[:data_slice,:]
y_test = y_test[:data_slice,:]

y_train.shape

model = keras.Sequential()
model.add(layers.Dense(64, activation='relu', input_dim=784))
model.add(layers.Dense(128, activation='relu', input_dim=64))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(256, activation='relu', input_dim=128))
model.add(layers.Dense(512, activation='relu', input_dim=256))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(256, activation='relu', input_dim=512))
model.add(layers.Dense(128, activation='relu', input_dim=256))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(64, activation='relu', input_dim=128))
model.add(layers.Dense(10, activation='softmax'))

model.summary()

model.compile(optimizer='adam',loss = ['binary_crossentropy'], metrics=['acc'])

Results = model.fit(x_train, y_train, validation_split=0.2, batch_size=64, epochs=20)

import matplotlib.pyplot as plt

plt.plot(Results.history['loss'])
plt.plot(Results.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(['Training','Validation'], loc = 'upper left')
plt.show()


plt.plot(Results.history['acc'])
plt.plot(Results.history['val_acc'])
plt.title('Accuracy')
plt.ylabel('Acc')
plt.xlabel('epochs')
plt.legend(['Training','Validation'], loc = 'upper left')
plt.show()