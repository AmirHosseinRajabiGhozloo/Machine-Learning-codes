{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP8o5OPloELm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npugyyzJoedc"
      },
      "outputs": [],
      "source": [
        "# 1. Import Required Modules\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import keras\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "import keras.backend as k\n",
        "from keras.models import *\n",
        "from keras.optimizers import *\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread, imshow, imsave, imread_collection\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy-z7-m_okVQ"
      },
      "outputs": [],
      "source": [
        "# 2. Define Train & Test Path (Images + Mask Path for Train and Test Stages)\n",
        "\n",
        "TRAIN_IMAGE_PATH = 'gdrive/My Drive/Berea_Sand_Texas/Image_Berea_512'\n",
        "TRAIN_MASK_PATH = 'gdrive/My Drive/Berea_Sand_Texas/Mask_Berea_512_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okWdWBsContp"
      },
      "outputs": [],
      "source": [
        "# 3. Initialize Images and Mask Size\n",
        "\n",
        "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 512, 512, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH3TCbIsoq7J"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "\n",
        "Train_Input = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"gdrive/My Drive/Parker_Sand/Image_512x512/*.png\"))]\n",
        "Train_Mask = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"gdrive/My Drive/Parker_Sand/Label_512x512/*.png\"))]\n",
        "\n",
        "Train_Input = np.array(Train_Input)\n",
        "Train_Mask = np.array(Train_Mask)\n",
        "\n",
        "Train_Mask = cv2.normalize(Train_Mask, None, alpha=1,beta=0, norm_type=cv2.NORM_MINMAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRLrHtsWot3o"
      },
      "outputs": [],
      "source": [
        "Train_Mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZmGc2Keoxxo"
      },
      "outputs": [],
      "source": [
        "imshow(Train_Mask[20], cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qd6bQZyo0V_"
      },
      "outputs": [],
      "source": [
        "Train_Mask[28].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaMzXv6Wo3-X"
      },
      "outputs": [],
      "source": [
        "print('Training Input')\n",
        "imshow(Train_Input[45])\n",
        "plt.show()\n",
        "\n",
        "print('Training Mask')\n",
        "imshow(np.squeeze(Train_Mask[45]), cmap='Greys_r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip2zfBrto7if"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.math.sigmoid(y_pred)\n",
        "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
        "    denominator = tf.reduce_sum(y_true + y_pred)\n",
        "\n",
        "    return 1 - numerator / denominator\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = k.flatten(y_true)\n",
        "    y_pred_f = k.flatten(y_pred)\n",
        "    intersection = k.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (k.sum(y_true_f) + k.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "def iou_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = k.sum(k.abs(y_true * y_pred), axis=[1,2])\n",
        "  union = k.sum(y_true,[1,2])+k.sum(y_pred,[1,2])-intersection\n",
        "  iou = k.mean((intersection+smooth) / (union+smooth), axis=0)\n",
        "  return iou\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = k.sum(k.round(k.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = k.sum(k.round(k.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + k.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = k.sum(k.round(k.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = k.sum(k.round(k.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + k.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "   precision = precision_m(y_true, y_pred)\n",
        "   recall = recall_m(y_true, y_pred)\n",
        "   return 2*((precision*recall)/(precision+recall+k.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqnGJegwo_qY"
      },
      "outputs": [],
      "source": [
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "    def __init__(self, pool_size=(2, 2), strides=(2, 2), padding=\"same\", **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "        self.padding = padding\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        padding = self.padding\n",
        "        pool_size = self.pool_size\n",
        "        strides = self.strides\n",
        "        if k.backend() == \"tensorflow\":\n",
        "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
        "            padding = padding.upper()\n",
        "            strides = [1, strides[0], strides[1], 1]\n",
        "            output, argmax = k.tf.nn.max_pool_with_argmax(\n",
        "                inputs, ksize=ksize, strides=strides, padding=padding\n",
        "            )\n",
        "        else:\n",
        "            errmsg = \"{} backend is not supported for layer {}\".format(\n",
        "                k.backend(), type(self).__name__\n",
        "            )\n",
        "            raise NotImplementedError(errmsg)\n",
        "        argmax = k.cast(argmax, k.floatx())\n",
        "        return [output, argmax]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ratio = (1, 2, 2, 1)\n",
        "        output_shape = [\n",
        "            dim // ratio[idx] if dim is not None else None\n",
        "            for idx, dim in enumerate(input_shape)\n",
        "        ]\n",
        "        output_shape = tuple(output_shape)\n",
        "        return [output_shape, output_shape]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return 2 * [None]\n",
        "\n",
        "\n",
        "class MaxUnpooling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        with k.tf.compat.v1.variable_scope(self.name):\n",
        "            mask = k.cast(mask, \"int32\")\n",
        "            input_shape = k.tf.shape(updates, out_type=\"int32\")\n",
        "            #  calculation new shape\n",
        "            if output_shape is None:\n",
        "                output_shape = (\n",
        "                    input_shape[0],\n",
        "                    input_shape[1] * self.size[0],\n",
        "                    input_shape[2] * self.size[1],\n",
        "                    input_shape[3],\n",
        "                )\n",
        "            self.output_shape1 = output_shape\n",
        "\n",
        "            # calculation indices for batch, height, width and feature maps\n",
        "            one_like_mask = k.ones_like(mask, dtype=\"int32\")\n",
        "            batch_shape = k.concatenate([[input_shape[0]], [1], [1], [1]], axis=0)\n",
        "            batch_range = k.reshape(\n",
        "                k.tf.range(output_shape[0], dtype=\"int32\"), shape=batch_shape\n",
        "            )\n",
        "            b = one_like_mask * batch_range\n",
        "            y = mask // (output_shape[2] * output_shape[3])\n",
        "            x = (mask // output_shape[3]) % output_shape[2]\n",
        "            feature_range = k.tf.range(output_shape[3], dtype=\"int32\")\n",
        "            f = one_like_mask * feature_range\n",
        "\n",
        "            # transpose indices & reshape update values to one dimension\n",
        "            updates_size = k.tf.size(updates)\n",
        "            indices = k.transpose(k.reshape(k.stack([b, y, x, f]), [4, updates_size]))\n",
        "            values = k.reshape(updates, [updates_size])\n",
        "            ret = k.tf.scatter_nd(indices, values, output_shape)\n",
        "            return ret\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        return (\n",
        "            mask_shape[0],\n",
        "            mask_shape[1] * self.size[0],\n",
        "            mask_shape[2] * self.size[1],\n",
        "            mask_shape[3],\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlG48yGopL6g"
      },
      "outputs": [],
      "source": [
        "def U_Seg_Net_2D_Segmentation(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "    n = Lambda(lambda x:x/255)(inputs)\n",
        "\n",
        "\n",
        "    c1 = Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer=tf.keras.initializers.Ones())(n)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(16, (3,3), activation='relu', padding='same')(c1)\n",
        "\n",
        "\n",
        "    #p1 = MaxPooling2D((2,2))(c1)\n",
        "    p1, mask_1 = MaxPoolingWithArgmax2D((2,2))(c1)\n",
        "\n",
        "\n",
        "    c2 = Conv2D(32, (3,3), activation='relu', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(32, (3,3), activation='relu', padding='same')(c2)\n",
        "\n",
        "\n",
        "    #p2 = MaxPooling2D((2,2))(c2)\n",
        "    p2, mask_2 = MaxPoolingWithArgmax2D((2,2))(c2)\n",
        "\n",
        "\n",
        "    c3 = Conv2D(64, (3,3), activation='relu', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3,3), activation='relu', padding='same')(c3)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3,3), activation='relu', padding='same')(c3)\n",
        "\n",
        "\n",
        "    #p3 = MaxPooling2D((2,2))(c3)\n",
        "    p3, mask_3 = MaxPoolingWithArgmax2D((2,2))(c3)\n",
        "\n",
        "\n",
        "    c4 = Conv2D(128, (3,3), activation='relu', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3,3), activation='relu', padding='same')(c4)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3,3), activation='relu', padding='same')(c4)\n",
        "\n",
        "    #p4 = MaxPooling2D((2,2))(c4)\n",
        "    p4, mask_4 = MaxPoolingWithArgmax2D((2,2))(c4)\n",
        "\n",
        "\n",
        "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(p4)\n",
        "    c5 = Dropout(0.2)(c5)\n",
        "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(c5)\n",
        "    c5 = Dropout(0.2)(c5)\n",
        "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(c5)\n",
        "\n",
        "\n",
        "    p5, mask_5 = MaxPoolingWithArgmax2D((2,2))(c5)\n",
        "\n",
        "\n",
        "    #u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
        "    #u6 = concatenate([u6, c4])\n",
        "    up1 = MaxUnpooling2D((2,2))([p5, mask_5])\n",
        "\n",
        "\n",
        "    u6 = concatenate([up1, c5])\n",
        "\n",
        "\n",
        "    c6 = Conv2D(256, (3,3), activation='relu', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(256, (3,3), activation='relu', padding='same')(c6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3,3), activation='relu', padding='same')(c6)\n",
        "\n",
        "\n",
        "    #u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
        "    #u7 = concatenate([u7, c3])\n",
        "    up2 = MaxUnpooling2D((2,2))([c6, mask_4])\n",
        "\n",
        "\n",
        "    u7 = concatenate([up2, c4])\n",
        "\n",
        "\n",
        "    c7 = Conv2D(128, (3,3), activation='relu', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(128, (3,3), activation='relu', padding='same')(c7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3,3), activation='relu', padding='same')(c7)\n",
        "\n",
        "\n",
        "    #u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\n",
        "    #u8 = concatenate([u8, c2])\n",
        "    up3 = MaxUnpooling2D((2,2))([c7, mask_3])\n",
        "\n",
        "\n",
        "    u8 = concatenate([up3, c3])\n",
        "\n",
        "\n",
        "    c8 = Conv2D(64, (3,3), activation='relu', padding='same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(64, (3,3), activation='relu', padding='same')(c8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(32, (3,3), activation='relu', padding='same')(c8)\n",
        "\n",
        "\n",
        "    #u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\n",
        "    #u9 = concatenate([u9, c1], axis = 3)\n",
        "    up4 = MaxUnpooling2D((2,2))([c8, mask_2])\n",
        "\n",
        "\n",
        "    u9 = concatenate([up4, c2])\n",
        "\n",
        "\n",
        "    c9 = Conv2D(32, (3,3), activation='relu', padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(16, (3,3), activation='relu', padding='same')(c9)\n",
        "\n",
        "\n",
        "    up5 = MaxUnpooling2D((2,2))([c9, mask_1])\n",
        "\n",
        "\n",
        "    u10 = concatenate([up5, c1])\n",
        "\n",
        "\n",
        "    c10 = Conv2D(16, (3,3), activation='relu', padding='same')(u10)\n",
        "\n",
        "\n",
        "    outputs = Conv2D(1,(1,1), activation='sigmoid')(c10)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss = [dice_loss], metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0, 1]), tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0]), tf.keras.metrics.IoU(num_classes=2, target_class_ids=[1]), f1_m ,precision_m , recall_m, 'accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejxHOlA3pQTp"
      },
      "outputs": [],
      "source": [
        "model = U_Seg_Net_2D_Segmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_AU6FkYpUGo"
      },
      "outputs": [],
      "source": [
        "# 7. Show The Results per Epoch\n",
        "\n",
        "class loss_history(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__ (self, x=4):\n",
        "        self.x = x\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "\n",
        "        imshow(np.squeeze(Train_Input[self.x]))\n",
        "        plt.show()\n",
        "\n",
        "        imshow(np.squeeze(Train_Mask[self.x]), cmap='Greys_r')\n",
        "        plt.show()\n",
        "\n",
        "        preds_train = self.model.predict(np.expand_dims(Train_Input[self.x], axis = 0))\n",
        "        imshow(np.squeeze(preds_train[0]), cmap='Greys_r')\n",
        "        plt.show()\n",
        "\n",
        "Model_Path = 'gdrive/My Drive/Saved_Models/2D_U_Seg_Net/Parker_IoU_Modified_U_Seg_Net'\n",
        "\n",
        "earlystopper = EarlyStopping(patience=7, verbose=1)\n",
        "checkpointer = ModelCheckpoint(Model_Path, verbose = 1, save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Validation_Input = Train_Input[:200]\n",
        "Train_Input = Train_Input[200:]\n",
        "Validation_Mask = Train_Mask[:200]\n",
        "Train_Mask = Train_Mask[200:]"
      ],
      "metadata": {
        "id": "yLK8RRIG6qVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2I8GNsspXzO"
      },
      "outputs": [],
      "source": [
        "# 8. Train U_NET Model using Training Samples\n",
        "\n",
        "results = model.fit(Train_Input, Train_Mask,\n",
        "                    validation_data=(Validation_Input, Validation_Mask),\n",
        "                    batch_size=10,\n",
        "                    epochs=150\n",
        "                    , callbacks=[earlystopper, checkpointer, loss_history()])\n",
        "#earlystopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tuzYi0A1chL"
      },
      "outputs": [],
      "source": [
        "preds_train = model.predict(Train_Input, verbose=1)\n",
        "preds_train_t = (preds_train>0.5).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_Mask = np.expand_dims(Train_Mask, axis=3)"
      ],
      "metadata": {
        "id": "A7uZpohcP20Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IoU calculation\n",
        "intersection = np.logical_and(Train_Mask, preds_train)\n",
        "union = np.logical_or(Train_Mask, preds_train)\n",
        "iou_score = np.sum(intersection) / np.sum(union)"
      ],
      "metadata": {
        "id": "busaOUI2lVsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iou_score"
      ],
      "metadata": {
        "id": "-NtoC8-SmXJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcVbtlpV1hAR"
      },
      "outputs": [],
      "source": [
        "# 10. Show Final Results (Segmented Images)\n",
        "\n",
        "ix = random.randint(0, len(Train_Input)-1)\n",
        "\n",
        "print(ix)\n",
        "\n",
        "print('Train_Image')\n",
        "imshow(Train_Input[ix])\n",
        "plt.show()\n",
        "\n",
        "print('Train_Mask')\n",
        "imshow(np.squeeze(Train_Mask[ix]), cmap='Greys_r')\n",
        "plt.show()\n",
        "\n",
        "print('Segmented_Image')\n",
        "imshow(np.squeeze(preds_train[ix]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcTI8BuQ1sYe"
      },
      "outputs": [],
      "source": [
        "# 11. Show Loss and ACC Plots\n",
        "\n",
        "\n",
        "# 11.1. Summarize History for Loss\n",
        "\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['Training','Validation'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 11.1. Summarize History for IOU\n",
        "\n",
        "plt.plot(results.history['io_u'])\n",
        "plt.plot(results.history['val_io_u'])\n",
        "plt.title('iou_coef')\n",
        "plt.ylabel('IOU')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['Training','Validation'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "# 11.1. Summarize History for IOU\n",
        "\n",
        "plt.plot(results.history['io_u_1'])\n",
        "plt.plot(results.history['val_io_u_1'])\n",
        "plt.title('iou_coef')\n",
        "plt.ylabel('IOU')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['Training','Validation'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob70DNBG10gp"
      },
      "outputs": [],
      "source": [
        "Test_Input = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"gdrive/My Drive/Image_512x512/*.png\"))]\n",
        "Test_Mask = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"gdrive/My Drive/Label_512x512/*.png\"))]\n",
        "\n",
        "Test_Input = np.array(Test_Input)\n",
        "Test_Mask = np.array(Test_Mask)\n",
        "\n",
        "Test_Mask = cv2.normalize(Test_Mask, None, alpha=1,beta=0, norm_type=cv2.NORM_MINMAX)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Results = model.evaluate(Test_Input, Test_Mask, batch_size=10)"
      ],
      "metadata": {
        "id": "rW4aG3lAQega"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_test = model.predict(Test_Input, verbose=1)\n",
        "preds_test_t = (preds_test>0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "nyn3cJ6XT8OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iix = random.randint(0, len(Test_Input)-1)\n",
        "\n",
        "print(ix)\n",
        "\n",
        "print('Test_Image')\n",
        "imshow(Test_Input[iix])\n",
        "plt.show()\n",
        "\n",
        "print('Test_Mask')\n",
        "imshow(np.squeeze(Test_Mask[iix]), cmap='Greys_r')\n",
        "plt.show()\n",
        "\n",
        "print('Segmented_Image')\n",
        "imshow(np.squeeze(preds_test[iix]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrSCAW8WT9Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Diff_Image = cv2.subtract(preds_test_t[215], Test_Mask[215])"
      ],
      "metadata": {
        "id": "6mjUjsr1UDg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Difference Image')\n",
        "imshow(np.squeeze(Diff_Image), cmap='Greys')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tKAtmchDUUTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}